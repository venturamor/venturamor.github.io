<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Mor Ventura</title>

    <meta name="author" content="Mor Ventura">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/rubber_duck_favicon_transparent.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:45%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Mor Ventura
                </p>
                <p>Hello there! I'm a PhD candidate under the supervision of <a href="https://roireichart.com/">Prof. Roi Reichart</a> in the <a href="https://www.technion.ac.il/en/home-2/">Technion</a>.
                </p>
                <p>
                  I am interested in NLP, Multimodality, Reasoning and Multilinguality. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:venturamor78@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=6uaBnRAAAAAJ&hl=iw">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/mor_ventura95">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/venturamor/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/linkedin_im_hq_morv.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/linkedin_im_hq_morv.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am deeply interested in the intersection of vision and language, as well as commonsense reasoning. My recent research focuses on challenging the visual reasoning capabilities of models, particularly in abductive reasoning. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="nleye_stop()" onmouseover="nleye_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nleye_image'>
					  <img src='images/nleye_im.png' width=100%>
					</div>
          <img src='images/nleye_im.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('nleye_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('nleye_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ever.github.io/">
			<span class="papertitle">NL-EYE: ABDUCTIVE NLI FOR IMAGES
</span>
        </a>
        <br>
				<a href="https://venturamor.github.io//"><strong>Mor Ventura</strong></a>, 
				<a href="https://tokeron.github.io/">Michael Toker</a>,
				<a href="https://nitaytech.github.io/">Nitay Calderon</a>,
        <a href="https://scholar.google.com/citations?user=c748UcIAAAAJ&hl=en">Zorik Gehkman</a>,
        <a href="https://yonatanbitton.github.io/">Yonatan Bitton</a>,
        <a href="https://roireichart.com/">Roi Reichart</a>,
				<br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://venturamor.github.io/NLEye/">project page</a>
        /
        <a href="https://arxiv.org/abs/2410.02613">arXiv</a>
        <p></p>
        <p>
          Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. 
        </p>
      </td>
    </tr>

    <tr onmouseout="cultext2i_stop()" onmouseover="cultext2i_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cultext2i'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cultext2i_city.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cultext2i_city.png' width="160">
        </div>
        <script type="text/javascript">
          function cultext2i_start() {
            document.getElementById('cultext2i').style.opacity = "1";
          }

          function cultext2i_stop() {
            document.getElementById('cultext2i').style.opacity = "0";
          }
          cultext2i_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://venturamor.github.io/CulText2IWeb/">
			<span class="papertitle">Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models
</span>
        </a>
        <br>
				<a href="https://venturamor.github.io/"><strong>Mor Ventura</strong></a>,
        <a href="https://eyalbd2.github.io/">Eyal Ben David</a>, 
        <a href="https://sites.google.com/site/annakorhonen/">Anna Korhonen</a>,
        <a href="https://roireichart.com/">Roi Reichart</a>, 
        <br>
        <em>TACL</em>, 2024 <font color="red"><strong>(TACL journal | ISCOL 2024 short talk)</strong></font>
        <br>
        <a href="https://venturamor.github.io/CulText2IWeb/">project page</a>
        /
        <a href="https://arxiv.org/abs/2310.01929">arXiv</a>
        <p></p>
        <p>
          Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have demonstrated remarkable prompt-based image generation capabilities. Multilingual encoders may have a substantial impact on the cultural agency of these models, as language is a conduit of culture. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. Based on this ontology, we derive prompt templates to unlock the cultural knowledge in TTI models, and propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model and human assessments, to evaluate the cultural content of TTI-generated images. To bolster our research, we introduce the CulText2I dataset, derived from six diverse TTI models and spanning ten languages. Our experiments provide insights regarding Do, What, Which and How research questions about the nature of cultural encoding in TTI models, paving the way for cross-cultural applications of these models.
        </p>
      </td>
    </tr>

    <tr onmouseout="diffusion_stop()" onmouseover="diffusion_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='diffusion_lens'><video  width=100% muted autoplay loop>
          <source src="images/diffusion_lens.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/diffusionLens.png' width=100%>
        </div>
        <script type="text/javascript">
          function diffusion_start() {
            document.getElementById('diffusion_lens').style.opacity = "1";
          }

          function diffusion_stop() {
            document.getElementById('diffusion_lens').style.opacity = "0";
          }
          diffusion_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://tokeron.github.io/DiffusionLensWeb/">
          <span class="papertitle">Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines</span>
        </a>
        <br>
        <a href="https://tokeron.github.io/">Michael Toker</a>,
        <a href="https://orgadhadas.github.io/">Hadas Orgad</a>,
        <a href="https://venturamor.github.io/"><strong>Mor Ventura</strong></a>,
        <a href="https://scholar.google.com/citations?user=A363uwwAAAAJ&hl=en">Dana Arad</a>,
        <a href="https://belinkov.com/">Yonatan Belinkov</a>,
        <br>
        <em>ACL</em>, 2024
        <br>
        <a href="https://tokeron.github.io/DiffusionLensWeb/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hAZr8pjyt-Y&t=506s">video</a>
        /
        <a href="https://arxiv.org/abs/2403.05846">arXiv</a>
        <p></p>
        <p>
          Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts requires further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.
        </p>
      </td>
    </tr>


          </tbody></table>

<!--           
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cs188.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>
            
            
          </tbody></table> -->

          <section id="updates">
            <h2>Updates</h2>
            <ul>
                <li>
                    <strong>Oct 2024:</strong> I've received the 
                    <a href="https://che.org.il/scholarships/%D7%93%D7%95%D7%A7%D7%98%D7%95%D7%A8%D7%A0%D7%98%D7%99%D7%95%D7%AA-%D7%9E%D7%A6%D7%98%D7%99%D7%99%D7%A0%D7%95%D7%AA-%D7%91%D7%AA%D7%97%D7%95%D7%9E%D7%99-%D7%94%D7%94%D7%99%D7%99%D7%98%D7%A7/" target="_blank">
                        VATAT Scholarship for outstanding women doctoral students in high-tech fields
                    </a>.
                </li>
                <li>
                    <strong>Sep 2024:</strong> Outstanding TA Award for excellence as a Teaching Assistant in a Python course, CS Faculty.
                </li>
            </ul>
        </section>


        <section id="about-me">
          <h2>Just a Bit About Me 🐾🎥📚🌍</h2>
          <p>
              Hi there! Beyond my research, I’m an animal lover and proud companion to the adorable fluffball: 
              <a href="images/Guddi.jpg" target="_blank">
                <img src="images/Guddi.jpg" alt="Guddi" class="round-image"> Guddi the cat
            </a>.
            
            <style>
                .round-image {
                    width: 100px; /* Adjust size as needed */
                    height: 100px; /* Ensure the height matches the width */
                    border-radius: 50%; /* Makes the image circular */
                    object-fit: cover; /* Ensures the image scales properly within the circle */
                    border: 2px solid #ccc; /* Optional: adds a border for better visibility */
                }
            </style>
          </p>
          <p>
            I’m a big fan of food 🍲, running 🏃‍♂️, movies 🎥, books 📚, and traveling 🌍✈️.
          </p>
          <p>
              <!-- Life is all about balancing the serious stuff with the things you love—and these are just a few of mine! -->
          </p>
      </section>
        



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
